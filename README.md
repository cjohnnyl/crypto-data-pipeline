# ðŸª™ Crypto Data Pipeline

![Python](https://img.shields.io/badge/Python-3.9+-blue?style=for-the-badge&logo=python&logoColor=white)
![Apache Airflow](https://img.shields.io/badge/Airflow-2.7-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Container-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![dbt](https://img.shields.io/badge/dbt-Core-FF694B?style=for-the-badge&logo=dbt&logoColor=white)
![PySpark](https://img.shields.io/badge/PySpark-3.0+-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)

### ðŸ“– Sobre o Projeto

O **Crypto Data Pipeline** Ã© uma soluÃ§Ã£o *end-to-end* de Engenharia de Dados desenvolvida para extrair, processar e analisar a volatilidade do mercado de criptomoedas.

O objetivo deste projeto Ã© demonstrar a implementaÃ§Ã£o de uma **Arquitetura de Lakehouse Moderna** (Medallion Architecture) utilizando ferramentas *open source* e padrÃµes de mercado, simulando um ambiente de produÃ§Ã£o escalÃ¡vel e resiliente.

---

### ðŸ— Arquitetura & Tech Stack

O pipeline segue o fluxo **ELT (Extract, Load, Transform)**, garantindo a separaÃ§Ã£o entre processamento e armazenamento.

| Camada | Tecnologia | FunÃ§Ã£o |
| :--- | :--- | :--- |
| **OrquestraÃ§Ã£o** | **Apache Airflow** | Gerenciamento de DAGs, dependÃªncias e monitoramento de falhas. |
| **IngestÃ£o** | **Python (Requests)** | ExtraÃ§Ã£o de dados da API pÃºblica (CoinGecko). |
| **Processamento** | **PySpark** | Processamento distribuÃ­do e conversÃ£o de formatos (JSON -> Parquet). |
| **Armazenamento** | **Data Lake Local** | Estrutura de diretÃ³rios simulando S3 (Bronze, Silver, Gold). |
| **TransformaÃ§Ã£o** | **dbt Core** | Modelagem dimensional, testes de qualidade (Data Quality) e documentaÃ§Ã£o. |
| **Infraestrutura** | **Docker** | ContainerizaÃ§Ã£o de todos os serviÃ§os. |

---

### ðŸ“‚ Estrutura do RepositÃ³rio

```text
crypto-data-pipeline/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/             # Pipelines de OrquestraÃ§Ã£o (ETL)
â”‚   â””â”€â”€ scripts/          # Scripts auxiliares (PySpark/Python)
â”œâ”€â”€ data/                 # Data Lake Local (SimulaÃ§Ã£o S3)
â”‚   â”œâ”€â”€ bronze/           # Raw Data (JSON)
â”‚   â”œâ”€â”€ silver/           # Cleaned Data (Parquet)
â”‚   â””â”€â”€ gold/             # Business Aggregates
â”œâ”€â”€ dbt_project/          # TransformaÃ§Ãµes SQL e Testes
â”œâ”€â”€ docker-compose.yml    # Infraestrutura como CÃ³digo
â””â”€â”€ requirements.txt      # DependÃªncias Python