# ğŸª™ Crypto Data Pipeline

![Python](https://img.shields.io/badge/Python-3.9+-blue?style=for-the-badge&logo=python&logoColor=white)
![Apache Airflow](https://img.shields.io/badge/Airflow-2.7-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Container-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![dbt](https://img.shields.io/badge/dbt-Core-FF694B?style=for-the-badge&logo=dbt&logoColor=white)
![PySpark](https://img.shields.io/badge/PySpark-3.0+-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)

### ğŸ“– Sobre o Projeto

O **Crypto Data Pipeline** Ã© uma soluÃ§Ã£o *end-to-end* de Engenharia de Dados desenvolvida para extrair, processar e analisar a volatilidade do mercado de criptomoedas.

O objetivo deste projeto Ã© demonstrar a implementaÃ§Ã£o de uma **Arquitetura de Lakehouse Moderna** (Medallion Architecture) utilizando ferramentas *open source* e padrÃµes de mercado, simulando um ambiente de produÃ§Ã£o escalÃ¡vel e resiliente.

---

### ğŸ— Arquitetura & Tech Stack

O pipeline segue o fluxo **ELT (Extract, Load, Transform)**, garantindo a separaÃ§Ã£o entre processamento e armazenamento.

| Camada | Tecnologia | FunÃ§Ã£o |
| :--- | :--- | :--- |
| **OrquestraÃ§Ã£o** | **Apache Airflow** | Gerenciamento de DAGs, dependÃªncias e monitoramento de falhas. |
| **IngestÃ£o** | **Python (Requests)** | ExtraÃ§Ã£o de dados da API pÃºblica (CoinGecko). |
| **Processamento** | **PySpark** | Processamento distribuÃ­do e conversÃ£o de formatos (JSON -> Parquet). |
| **Armazenamento** | **Data Lake Local** | Estrutura de diretÃ³rios simulando S3 (Bronze, Silver, Gold). |
| **TransformaÃ§Ã£o** | **dbt Core** | Modelagem dimensional, testes de qualidade (Data Quality) e documentaÃ§Ã£o. |
| **Infraestrutura** | **Docker** | ContainerizaÃ§Ã£o de todos os serviÃ§os. |

---

### ğŸ“‚ Estrutura do RepositÃ³rio

```text
crypto-data-pipeline/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/             # Pipelines de OrquestraÃ§Ã£o (ETL)
â”‚   â””â”€â”€ scripts/          # Scripts auxiliares (PySpark/Python)
â”œâ”€â”€ data/                 # Data Lake Local (SimulaÃ§Ã£o S3)
â”‚   â”œâ”€â”€ bronze/           # Raw Data (JSON)
â”‚   â”œâ”€â”€ silver/           # Cleaned Data (Parquet)
â”‚   â””â”€â”€ gold/             # Business Aggregates
â”œâ”€â”€ dbt_project/          # TransformaÃ§Ãµes SQL e Testes
â”œâ”€â”€ docker-compose.yml    # Infraestrutura como CÃ³digo
â””â”€â”€ requirements.txt      # DependÃªncias Python
---

### ğŸš€ Como Executar

#### ğŸ“‹ PrÃ©-requisitos

- Docker Engine 20.10+ (ou Docker Desktop)
- Docker Compose 2.0+
- Make (opcional, para automaÃ§Ã£o de comandos)
- 4GB de RAM disponÃ­vel para os containers

#### âš™ï¸ Setup e ExecuÃ§Ã£o

**OpÃ§Ã£o 1: Usando Make (Recomendado)**

```bash
# Clone o repositÃ³rio
git clone <repository-url>
cd crypto-data-pipeline

# Inicie o ambiente completo
make up

# Para parar os containers
make down

# Para limpar volumes e remover tudo
make clean
```

**OpÃ§Ã£o 2: Usando Docker Compose direto**

```bash
# Clone o repositÃ³rio
git clone <repository-url>
cd crypto-data-pipeline

# Configure o ambiente
echo "AIRFLOW_UID=50000" > .env
mkdir -p airflow/logs airflow/plugins airflow/dags data/bronze dbt_project/logs scripts

# Ajuste permissÃµes (Linux/macOS)
chmod -R 777 airflow data dbt_project scripts

# Inicie os containers
docker compose up -d --build

# Para parar
docker compose down
```

**Windows (PowerShell)**

```powershell
# Configure o ambiente
"AIRFLOW_UID=50000" | Out-File -FilePath .env -Encoding ASCII
New-Item -ItemType Directory -Force -Path airflow/logs, airflow/plugins, airflow/dags, data/bronze, dbt_project/logs, scripts

# Inicie os containers
docker compose up -d --build
```

#### ğŸŒ Acessando o Airflow

ApÃ³s a inicializaÃ§Ã£o (aguarde ~2 minutos):

- **URL**: http://localhost:8080
- **UsuÃ¡rio**: `airflow`
- **Senha**: `airflow`

---

### ğŸ”§ Troubleshooting

**Erro de permissÃ£o em logs:**
```bash
# Linux/macOS
chmod -R 777 airflow data dbt_project scripts

# Ou remover logs antigos
rm -rf airflow/logs/*
```

**Containers nÃ£o iniciam:**
```bash
# Verificar logs
docker compose logs -f

# Recriar containers
docker compose down --volumes
docker compose up -d --build
```
